{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Hands-on AI II</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Unit 5 – Language Modeling with LSTM </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authors:</b> N. Rekabsaz, B. Schäfl, S. Lehner, J. Brandstetter, E. Kobler<br>\n",
    "<b>Date:</b> 11-05-2022\n",
    "\n",
    "This file is part of the \"Hands-on AI II\" lecture material. The following copyright statement applies to all code within this file.\n",
    "\n",
    "<b>Copyright statement:</b><br>\n",
    "This material, no matter whether in printed or electronic form, may be used for personal and non-commercial educational use only. Any reproduction of this material, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of contents</h2>\n",
    "<ol>\n",
    "    <a href=\"#lm\"><li style=\"font-size:large;font-weight:bold\">Language Model Training and Evaluation</li></a>\n",
    "    <ol style=\"margin-bottom:15px\">\n",
    "        <a href=\"#lm-parameters\"><li style=\"font-size:medium\">Defining Parameters</li></a>\n",
    "        <a href=\"#lm-data\"><li style=\"font-size:medium\">Data & Dictionary Preparation</li></a>\n",
    "        <a href=\"#lm-model\"><li style=\"font-size:medium\">Model Definition</li></a>\n",
    "        <a href=\"#lm-training\"><li style=\"font-size:medium\">Training & Evaluation</li></a>\n",
    "    </ol>\n",
    "    <a href=\"#generation\"><li style=\"font-size:large;font-weight:bold\">Language Generation</li></a>\n",
    "    \n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">How to use this notebook</h3>\n",
    "<p><p>This notebook is designed to run from start to finish. There are different tasks (displayed in <span style=\"color:rgb(248,138,36)\">orange boxes</span>) which might require small code modifications. Most/All of the used functions are imported from the file <code>u5_utils.py</code> which can be seen and treated as a black box. However, for further understanding, you can look at the implementations of the helper functions. In order to run this notebook, the packages which are imported at the beginning of <code>u5_utils.py</code> need to be installed.</p></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .output_png {\n",
       "            display: table-cell;\n",
       "            text-align: center;\n",
       "            vertical-align: middle;\n",
       "        }\n",
       "        .jp-RenderedImage {\n",
       "            display: table-cell;\n",
       "            text-align: center;\n",
       "            vertical-align: middle;\n",
       "        }\n",
       "    </style>\n",
       "    <p>Setting up notebook ... finished.</p>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import u5_utils as u5\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import ipdb\n",
    "\n",
    "# Setup Jupyter notebook (warning: this may affect all Jupyter notebooks running on the same Jupyter server).\n",
    "u5.setup_jupyter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Module versions</h3>\n",
    "<p><p>As mentioned in the introductory slides, specific minimum versions of Python itself as well as of used modules is recommended.</p></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed Python version: 3.8 (✓)\n",
      "Installed numpy version: 1.22.2 (✓)\n",
      "Installed pandas version: 1.4.1 (✓)\n",
      "Installed PyTorch version: 1.10.2+cu113 (✓)\n"
     ]
    }
   ],
   "source": [
    "u5.check_module_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"lm\"></a><h2>Language Model Training and Evaluation</h2>\n",
    "<p><p>In this section, we will create a language model with LSTM, trained on the words of a text corpus and evaluated on a hold-out set.\n",
    "In detail, we use the Penn parsed corpus:\n",
    "\n",
    "<center><cite>\n",
    "Seth Kulick, Anthony Kroch, and Beatrice Santorini. 2014. The Penn Parsed Corpus of Modern British English: First Parsing Results and Analysis. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 662–667, Baltimore, Maryland. Association for Computational Linguistics.\n",
    "</cite></center>\n",
    "\n",
    "</p></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "## Parameters setting\n",
    "\n",
    "## input & output parameters\n",
    "data_path = os.path.join(\"resources\", \"penn\")\n",
    "save_path = 'model.pt' # path to save the final model\n",
    "\n",
    "## training & evaluation parameters\n",
    "train_batch_size = 32 # batch size for training\n",
    "eval_batch_size = 32 # batch size for elidation/test\n",
    "max_seq_len = 40 # sequence length\n",
    "\n",
    "seed = 1111 # random seed to facilitate reproducibility\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"lm-data\"></a><h3 style=\"color:rgb(0,120,170)\">Data & Dictionary Preperation</h3>\n",
    "<p><p>\n",
    "The train/val/test text corpora are loaded and tokenized. The provided data files are already pre-processed (cleaned). After loading, we create a dictionary based on the <i>training data</i>, which maps every word to a wordID. We then use the dictionary to map all the words in the data files to streams of wordIDs. Finally, the datasets are split into a set of sequences according to the given batch size.</p></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in dictionary 10001\n"
     ]
    }
   ],
   "source": [
    "train_corpus = u5.Corpus(os.path.join(data_path, 'train.txt'))\n",
    "valid_corpus = u5.Corpus(os.path.join(data_path, 'valid.txt'))\n",
    "test_corpus = u5.Corpus(os.path.join(data_path, 'test.txt'))\n",
    "\n",
    "dictionary = u5.Dictionary()\n",
    "train_corpus.fill_dictionary(dictionary)\n",
    "ntokens = len(dictionary)\n",
    "print(f'Number of tokens in dictionary {ntokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordID of a word in the dictionary: 1203\n",
      "A word in the dictionary based on its wordID: says\n"
     ]
    }
   ],
   "source": [
    "# some samples in the dictionary ...\n",
    "print('wordID of a word in the dictionary:', dictionary.word2idx['book'])\n",
    "print('A word in the dictionary based on its wordID:', dictionary.idx2word[854])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: number of tokens 929589\n",
      "Validation data: number of tokens 73760\n",
      "Test data: number of tokens 82430\n",
      "\n",
      "Train data split shape: torch.Size([29049, 32])\n",
      "Validation data split shape: torch.Size([2305, 32])\n",
      "Test data batchified shape: torch.Size([2575, 32])\n"
     ]
    }
   ],
   "source": [
    "train_data = train_corpus.words_to_ids(dictionary)\n",
    "print(f'Train data: number of tokens {len(train_data)}')\n",
    "\n",
    "valid_data = valid_corpus.words_to_ids(dictionary)\n",
    "print(f'Validation data: number of tokens {len(valid_data)}')\n",
    "\n",
    "test_data = test_corpus.words_to_ids(dictionary)\n",
    "print(f'Test data: number of tokens {len(test_data)}')\n",
    "\n",
    "print()\n",
    "train_data_splits = u5.batchify(train_data, train_batch_size, device)\n",
    "print(f'Train data split shape: {train_data_splits.shape}')\n",
    "\n",
    "val_data_splits = u5.batchify(valid_data, eval_batch_size, device)\n",
    "print(f'Validation data split shape: {val_data_splits.shape}')\n",
    "\n",
    "test_data_splits = u5.batchify(test_data, eval_batch_size, device)\n",
    "print(f'Test data batchified shape: {test_data_splits.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aer the designed to kim all president and situation by s. N this gain wright <unk> government detailing industry reported in open a the foreign expected employers to billion company fall of'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([dictionary.idx2word[i] for i in train_data_splits[0].tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"tasks-one\"></a><h3 style=\"color:rgb(0,120,170)\">Tasks</h3>\n",
    "    <div class=\"alert alert-warning\">\n",
    "        Execute the notebook until here and try to solve the following tasks:\n",
    "        <ul>\n",
    "            <li>Print the first 100 wordIDs of the 3rd sequence in <code>train_data_splits</code>.</li>\n",
    "            <li>Print the first wordIDs in all sequences in <code>train_data_splits</code>. What should be the shape of the resulting tensor?</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"lm-model\"></a><h3 style=\"color:rgb(0,120,170)\">Model Definition</h3>\n",
    "<p><p>Our language model consists of an encoder matrix, an LSTM, and a decoder matrix. The decoder matrix transfers the hidden states from the low-embedding-dimension to the dimension of the size of vocabularies. The overall scheme of the model is shown below:\n",
    "</p></p>\n",
    "\n",
    "<center>\n",
    "    <img src=\"resources/lm_lstm_model.png\" alt=\"Image not found!\" style=\"width: 50%;\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_LSTMModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid):\n",
    "        super(LM_LSTMModel, self).__init__()\n",
    "        self.ntoken = ntoken\n",
    "        self.encoder = torch.nn.Embedding(ntoken, ninp) # matrix E in the figure\n",
    "        self.rnn = torch.nn.LSTM(ninp, nhid)\n",
    "        self.decoder = torch.nn.Linear(nhid, ntoken) # matrix U in the figure\n",
    "\n",
    "        self.init_weights()\n",
    "        self.nhid = nhid\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(1, bsz, self.nhid),\n",
    "                weight.new_zeros(1, bsz, self.nhid))\n",
    "\n",
    "    def forward(self, input, hidden, return_logs=True):\n",
    "#         ipdb.set_trace()\n",
    "        emb = self.encoder(input)\n",
    "        hiddens, last_hidden = self.rnn(emb, hidden)\n",
    "        \n",
    "        decoded = self.decoder(hiddens)\n",
    "        if return_logs:\n",
    "            y_hat = torch.nn.LogSoftmax(dim=-1)(decoded)\n",
    "        else:\n",
    "            y_hat = torch.nn.Softmax(dim=-1)(decoded)\n",
    "\n",
    "        return y_hat, last_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LM_LSTMModel(\n",
      "  (encoder): Embedding(10001, 200)\n",
      "  (rnn): LSTM(200, 200)\n",
      "  (decoder): Linear(in_features=200, out_features=10001, bias=True)\n",
      ")\n",
      "Model total parameters: 4332001\n",
      "Model total trainable parameters: 4332001\n"
     ]
    }
   ],
   "source": [
    "## model parameters\n",
    "emsize = 200  # size of word embeddings\n",
    "nhid = 200  # number of hidden units per layer\n",
    "\n",
    "model = LM_LSTMModel(ntokens, emsize, nhid)\n",
    "model.to(device)\n",
    "\n",
    "print(f'Model: {model}')\n",
    "print(f'Model total parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "print(f'Model total trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"tasks-one\"></a><h3 style=\"color:rgb(0,120,170)\">Tasks</h3>\n",
    "    <div class=\"alert alert-warning\">\n",
    "        Execute the notebook until here and try to solve the following tasks:\n",
    "        <ul>\n",
    "            <li>Considering the provided figure, find the corresponding components of the language model in the <code>\\_\\_init\\_\\_</code> function of <code>LM_LSTMModel</code>.</li>\n",
    "            <li>Read the <code>forward</code> method of <code>LM_LSTMModel</code> and try to follow the data flow of the language model (from input to output) as shown in the figure. </li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"lm-training\"></a><h3 style=\"color:rgb(0,120,170)\">Training and Evaluation</h3>\n",
    "<p><p></p></p>\n",
    "\n",
    "This section contains the code of training the model and evaluating the validation set. Performance is evaluated with [perplexity measure](https://en.wikipedia.org/wiki/Perplexity). Descriptions are provided as comments inside the code. The process of training is depicted below:\n",
    "\n",
    "<center>\n",
    "    <img src=\"resources/lm_training.png\" alt=\"Image not found!\" style=\"width: 50%;\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |    25/  726 batches | lr 20.00 | ms/batch 97.16 | loss  7.90 | perplexity  2693.29\n",
      "| epoch   0 |    50/  726 batches | lr 20.00 | ms/batch 64.68 | loss  6.56 | perplexity   704.55\n",
      "| epoch   0 |    75/  726 batches | lr 20.00 | ms/batch 65.16 | loss  6.26 | perplexity   523.42\n",
      "| epoch   0 |   100/  726 batches | lr 20.00 | ms/batch 64.99 | loss  6.14 | perplexity   465.38\n",
      "| epoch   0 |   125/  726 batches | lr 20.00 | ms/batch 64.62 | loss  6.02 | perplexity   411.57\n",
      "| epoch   0 |   150/  726 batches | lr 20.00 | ms/batch 64.73 | loss  5.89 | perplexity   362.08\n",
      "| epoch   0 |   175/  726 batches | lr 20.00 | ms/batch 72.30 | loss  5.99 | perplexity   399.14\n",
      "| epoch   0 |   200/  726 batches | lr 20.00 | ms/batch 64.85 | loss  5.91 | perplexity   366.96\n",
      "WARNING: Training is interrupted after 200 batches\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 15.29s| valid loss  5.87 | valid perplexity   355.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   1 |    25/  726 batches | lr 20.00 | ms/batch 67.15 | loss  6.15 | perplexity   469.74\n",
      "| epoch   1 |    50/  726 batches | lr 20.00 | ms/batch 64.82 | loss  5.76 | perplexity   317.99\n",
      "| epoch   1 |    75/  726 batches | lr 20.00 | ms/batch 64.71 | loss  5.72 | perplexity   303.49\n",
      "| epoch   1 |   100/  726 batches | lr 20.00 | ms/batch 65.14 | loss  5.69 | perplexity   294.82\n",
      "| epoch   1 |   125/  726 batches | lr 20.00 | ms/batch 64.50 | loss  5.63 | perplexity   279.34\n",
      "| epoch   1 |   150/  726 batches | lr 20.00 | ms/batch 64.61 | loss  5.53 | perplexity   252.54\n",
      "| epoch   1 |   175/  726 batches | lr 20.00 | ms/batch 65.04 | loss  5.64 | perplexity   281.75\n",
      "| epoch   1 |   200/  726 batches | lr 20.00 | ms/batch 64.05 | loss  5.61 | perplexity   273.84\n",
      "WARNING: Training is interrupted after 200 batches\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 14.23s| valid loss  5.71 | valid perplexity   301.59\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "CUT_AFTER_BATCHES = 200  # JUST FOR DEBUGGING: cut the loop after these number of batches. Set to -1 to ignore\n",
    "epochs = 2  # upper epoch limit\n",
    "print_interval = 25  # report interval\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module, optimizer: torch.optim.Optimizer, dictionary: u5.Dictionary,\n",
    "          max_seq_len: int, train_batch_size: int, train_data_splits,\n",
    "          clipping: float, learning_rate: float, print_interval: int, epoch: int):\n",
    "    \"\"\"\n",
    "    Train the model. Training mode turned on to enable dropout.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    ntokens = len(dictionary)\n",
    "    start_hidden = model.init_hidden(train_batch_size)\n",
    "    \n",
    "    for batch_i, i in enumerate(range(0, train_data_splits.size(0) - 1, max_seq_len)):\n",
    "        batch_data, batch_targets = u5.get_batch(train_data_splits, i, max_seq_len)\n",
    "        # ipdb.set_trace()\n",
    "\n",
    "        # Don't forget it! Otherwise, the gradients are summed together!\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Repackaging batches only keeps the value of start_hidden and disconnects its computational graph.\n",
    "        # If repackaging is not done the, gradients are calculated from the current point to the beginning\n",
    "        # of the sequence which becomes computationally too expensive.\n",
    "        start_hidden = u5.repackage_hidden(start_hidden)\n",
    "        \n",
    "        # forward pass\n",
    "        y_hat_logprobs, last_hidden = model(batch_data, start_hidden, return_logs=True)\n",
    "        \n",
    "        # loss computation & backward pass\n",
    "        y_hat_logprobs = y_hat_logprobs.view(-1, ntokens)\n",
    "        loss = torch.nn.NLLLoss()(y_hat_logprobs, batch_targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # The last hidden states of the current step is set as the start hidden state of the next step.\n",
    "        # This passes the information of the current batch to the next batch.\n",
    "        start_hidden = last_hidden\n",
    "\n",
    "        # clipping gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping)\n",
    "        \n",
    "        # updating parameters using SGD\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_i % print_interval == 0 and batch_i > 0:\n",
    "            cur_loss = total_loss / print_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | perplexity {:8.2f}'.format(\n",
    "                epoch, batch_i, (train_data_splits.size(0) - 1) // max_seq_len, learning_rate,\n",
    "                              elapsed * 1000 / print_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "        # cuts the loop (only for debugging)\n",
    "        if (CUT_AFTER_BATCHES != -1) and (batch_i >= CUT_AFTER_BATCHES):\n",
    "            print(\"WARNING: Training is interrupted after %d batches\" % batch_i)\n",
    "            break\n",
    "            \n",
    "\n",
    "lr = 20  # initial learning rate\n",
    "clipping = 0.25  # gradient clipping\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "best_val_loss = None\n",
    "\n",
    "# Loop over epochs.\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, optimizer, dictionary, max_seq_len, train_batch_size, train_data_splits, clipping, lr, print_interval, epoch)\n",
    "    val_loss = u5.evaluate(model, dictionary, max_seq_len, eval_batch_size, val_data_splits, torch.nn.NLLLoss())\n",
    "    \n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch :3d} | time: {time.time() - epoch_start_time :5.2f}s' \n",
    "          f'| valid loss {val_loss :5.2f} | valid perplexity {math.exp(val_loss):8.2f}')\n",
    "    print('-' * 89)\n",
    "    \n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        with open(save_path, 'wb') as f:\n",
    "            torch.save(model, f)\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finalizing the training, the best performing model (according to validation performance) is loaded and evaluated on the test corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| Test loss  5.66 | test perplexity 286.97\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model.\n",
    "with open(save_path, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    \n",
    "test_loss = u5.evaluate(model, dictionary, max_seq_len, eval_batch_size, test_data_splits, torch.nn.NLLLoss())\n",
    "print('=' * 89)\n",
    "print(f'| Test loss {test_loss :5.2f} | test perplexity {math.exp(test_loss) :5.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"tasks-one\"></a><h3 style=\"color:rgb(0,120,170)\">Tasks</h3>\n",
    "    <div class=\"alert alert-warning\">\n",
    "        Execute the notebook until here and try to solve the following tasks:\n",
    "        <ul>\n",
    "            <li>Using <code>ipdb</code> at the beginning of the loop in <code>train</code>, look at the <code>batch_data</code> and <code>batch_targets</code>. What are their shapes? What do they contain? How are they related to each other?</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"generation\"></a><h2>Language Generation</h2>\n",
    "<p><p>In this section, the trained model in the previous section is used to generate a sequence with a specific length. The language generation is done by sampling words from the predicted probability distribution of the language model. </p></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have stress without fields appointed in the strike against societe\n"
     ]
    }
   ],
   "source": [
    "GENERATION_LENGTH = 10\n",
    "START_WORD = \"I\"\n",
    "\n",
    "start_hidden = model.init_hidden(1)\n",
    "START_WORD = START_WORD.lower()\n",
    "    \n",
    "generated_text = START_WORD\n",
    "with torch.no_grad():\n",
    "    wordid_input = dictionary.word2idx[START_WORD]\n",
    "    for i in range(0, GENERATION_LENGTH):\n",
    "        data = u5.batchify(torch.tensor([wordid_input]), 1, device)\n",
    "\n",
    "        y_hat_probs, last_hidden = model(data, start_hidden, return_logs=False)\n",
    "        \n",
    "        prob_dist = torch.distributions.Categorical(y_hat_probs.squeeze())\n",
    "        wordid_input = prob_dist.sample()\n",
    "        word_generated = dictionary.idx2word[wordid_input]\n",
    "        \n",
    "        generated_text += \" \" + word_generated\n",
    "        \n",
    "        start_hidden = last_hidden\n",
    "        \n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"tasks-one\"></a><h3 style=\"color:rgb(0,120,170)\">Tasks</h3>\n",
    "    <div class=\"alert alert-warning\">\n",
    "        Execute the notebook until here and try to solve the following tasks:\n",
    "        <ul>\n",
    "            <li>For one of the steps, calculate the sum of the generated probability distribution (<code>prob_dist</code>). Is it equal to 1.0?</li>\n",
    "            <li>Change the length of the generated text. Does the text (still) remain coherent?</li>\n",
    "        </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1067ee20f23cf75b48768bdb5f7ec1d4c21e1831c972d070ed1f98bb55bb7e57"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
