{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68c2d19",
   "metadata": {},
   "source": [
    "Name | Matr.Nr. | Due Date\n",
    ":--- | ---: | ---:\n",
    "Firstname Lastname | 01234567 | 24.06.2022, 08:00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064bab9b",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Hands-on AI II</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Unit 6 &ndash; Introduction to Reinforcement Learning (Assignment)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c053aae",
   "metadata": {},
   "source": [
    "<b>Authors:</b> B. Sch√§fl, S. Lehner, J. Brandstetter, E. Kobler<br>\n",
    "<b>Date:</b> 30-05-2022\n",
    "\n",
    "This file is part of the \"Hands-on AI II\" lecture material. The following copyright statement applies to all code within this file.\n",
    "\n",
    "<b>Copyright statement:</b><br>\n",
    "This material, no matter whether in printed or electronic form, may be used for personal and non-commercial educational use only. Any reproduction of this material, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad5dc6",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">How to use this notebook</h3>\n",
    "\n",
    "This notebook is designed to run from start to finish. There are different tasks (displayed in <span style=\"color:rgb(248,138,36)\">orange boxes</span>) which require your contribution (in form of code, plain text, ...). Most/All of the supplied functions are imported from the file <code>u6_utils.py</code> which can be seen and treated as a black box. However, for further understanding, you can look at the implementations of the helper functions. In order to run this notebook, the packages which are imported at the beginning of <code>u6_utils.py</code> need to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fb7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pre-defined utilities specific to this notebook.\n",
    "import u6_utils as u6\n",
    "\n",
    "# Import additional utilities needed in this notebook.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import time\n",
    "import gym\n",
    "\n",
    "from IPython import display\n",
    "from typing import Tuple\n",
    "\n",
    "# Set default plotting style.\n",
    "sns.set()\n",
    "\n",
    "# Setup Jupyter notebook (warning: this may affect all Jupyter notebooks running on the same Jupyter server).\n",
    "u6.setup_jupyter()\n",
    "\n",
    "# Check minimum versions.\n",
    "u6.check_module_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768c14d1",
   "metadata": {},
   "source": [
    "<h2>Dissection of an Environment</h2>\n",
    "<p>All exercises in this assignment are referring to the <i>FrozenLake</i> environment of <a href=\"https://www.gymlibrary.ml/\"><i>OpenAI Gym</i></a>. This environment is described according to its official <a href=\"https://www.gymlibrary.ml/environments/toy_text/frozen_lake/\">OpenAI Gym website</a>.\n",
    "<center>\n",
    "    <cite>Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly <span style=\"color:rgb(0,255,0)\">frozen</span>, but there are a few <span style=\"color:rgb(255,0,0)\">holes</span> where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the <span style=\"color:rgb(255,0,255)\">disc</span>. However, the ice is slippery, so you won't always move in the direction you intend.</cite>\n",
    "    </center></p>\n",
    "\n",
    "\n",
    "<p>There are <i>four</i> types of surfaces described in this environment:\n",
    "<ul>\n",
    "    <li><code>S</code> $\\rightarrow$ starting point (<span style=\"color:rgb(0,255,0)\"><i>safe</i></span>)</li>\n",
    "    <li><code>F</code> $\\rightarrow$ frozen surface (<span style=\"color:rgb(0,255,0)\"><i>safe</i></span>)</li>\n",
    "    <li><code>H</code> $\\rightarrow$ hole (<span style=\"color:rgb(255,0,0)\"><i>fall to your doom</i></span>)</li>\n",
    "    <li><code>G</code> $\\rightarrow$ goal (<span style=\"color:rgb(255,0,255)\"><i>frisbee location</i></span>)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e142c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for creating a default FrozenLake environment (slippery is parameterized).\n",
    "def FrozenLakeEnv(slippery: bool = False) -> gym.Env:\n",
    "    return gym.make('FrozenLake-v1', desc=None, map_name=\"8x8\", is_slippery=slippery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b145d4f7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1. [5 Points]</b>\n",
    "    <ul>\n",
    "        <li>Create a <code>FrozenLakeEnv</code> with <code>slippery = False</code> and set the seed to $42$. Use this environment in the subsequent tasks if not specified otherwise. Render its current state in a human-readable way.</li>\n",
    "        <li>Gather and print the amount of different <i>actions</i> as well as <i>states</i> of the <code>FrozenLakeEnv</code> instance. Discuss the results.</li>\n",
    "        <li>Display the <i>reward table entry</i> for the current state. Discuss the different elements of the resulting dictionary.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff467d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774a152d",
   "metadata": {},
   "source": [
    "<h2>Tackling the Environment with Random Exploration</h2>\n",
    "<p>In the exercise, we talked about solving this kind of tasks in a na&#xEF;ve way by simply applying <i>brute force</i>: using <i>random search</i>. So far, we analyzed the <i>action</i> as well as the <i>state space</i> and came to the conclusion that such an approach is feasible. Here is the outline of such an approach:\n",
    "<ul>\n",
    "    <li><code>I</code> $\\rightarrow$ choose a random <i>action</i> with respect to the <i>current</i> state.</li>\n",
    "    <li><code>II</code> $\\rightarrow$ execute previously chosen <i>action</i> and transition into a <i>new</i> state.</li>\n",
    "    <li><code>III</code> $\\rightarrow$ if the episode is finished but the goal is not reached, <i>reset</i> the <i>environment</i>.</li>\n",
    "</ul>\n",
    "\n",
    "This procedure is repeated as long as the task is not solved or a defined <i>maximum number of steps</i> is reached, whatever triggers first (<code>IV</code>). Adapt the function <code>apply_random_search</code> as discussed during the exercise. Mark the corresponding sections of the code using <code>I</code>, <code>II</code>, <code>III</code> and <code>IV</code>. Note that our <i>random search</i> is <i>not</i> guaranteed to find  the solution of a task in <i>finite time</i>, hence an upper bound on the <i>runtime</i> is often applied as a safety net (in our case the <i>maximum number of steps</i>).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51118e65",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2.1 [10 Points]</b>\n",
    "    <ul>\n",
    "        <li>Implement the <i>random search</i> algorithm as outlined above (equivalently to the one discussed during the exercise).</li>\n",
    "        <li>Apply your random search implementation on a freshly $42$-seeded <code>FrozenLakeEnv</code> instance, with an animation delay of $0.01$ and a maximum number of steps of $500$.</li>\n",
    "        <li>Was the goal reached, how many steps were taken and how often did an involuntary dive happen?</li>\n",
    "        <li>Repeat the experiment and find parameters (random seed, maximum number of steps) that lead to a successful run (you do not need to animate this second experiment).</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc11b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67639955",
   "metadata": {},
   "source": [
    "<p>To drill down on the drawbacks of plain <i>random search</i>, we are designing the following experimental setup (<i>Hint</i>: it is actually the same experimental setup as already discussed during the exercise, so you might orient yourself on the implementation presented during class):\n",
    "<ul>\n",
    "    <li>Repeat the previous <i>random search</i> procedure a specified amount of times.</li>\n",
    "    <li>Aggregate the results of each run for later analysis.</li>\n",
    "    <li>Visualize the aggregated results using <i>box-</i> and <i>strip-plots</i> (or <i>swarm-plots</i>).</li>\n",
    "</ul>\n",
    "Once again, we are setting the <i>random seed</i>, but take care of setting it <i>outside</i> the loop, otherwise the same result is reported with each iteration (and an aggregation of the results would not give us any more insights).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046c4705",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2.2 [10 Points]</b>\n",
    "    <ul>\n",
    "        <li>Conduct a <i>random search experiment</i> as outlined above, using $100$ repetitions and the random seed set to $42$. Set the maximum number of steps to $10,000$.</li>\n",
    "        <li>Interpret the visualization (e.g. the span of the boxes) and keep the scaling of the <i>x-axis</i> in mind.</li>\n",
    "        <li>In comparison with the <code>Taxi</code> environment, what might be the problem with <code>FrozenLakeEnv</code> w.r.t. random exploration?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16713867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638bc008",
   "metadata": {},
   "source": [
    "<h2>Tackling the Environment with $Q$-Learning</h2>\n",
    "<p>In a simplified version of $Q$-learning, the <b>$\\boldsymbol{Q}$-value</b>\n",
    "\\begin{equation}\n",
    "    Q(s,a)\n",
    "\\end{equation}</p>\n",
    "\n",
    "<p>is the expected future reward of being in state $s$ and taking action $a$. Intuitively, if the $Q$-values are learned correctly, a good policy would be to take the action which maximizes the expected future reward. This is what $Q$-learning is doing. $Q$-learning lets the agent <b>use the environment's rewards to learn</b>, over time, the best action to take in a given state. $Q$-values are initialized to an arbitrary value, and as the agent exposes itself to the environment and receives different rewards by executing different actions, the $Q$-values are updated using the equation:\n",
    "\\begin{equation}\n",
    "    Q(s_t,a_t) \\leftarrow (1 - \\alpha) \\cdot Q(s_t,a_t) + \\alpha \\cdot \\left( r + \\gamma \\max_{a_{t+1}} Q(s_{t+1}, a_{t+1})\\right)\n",
    "\\end{equation}</p>\n",
    "\n",
    "<p>We are assigning $\\leftarrow$, or updating, the $Q$-value of the agent's current state and action, denoted as $Q(s_t,a_t)$ with $\\alpha$ as the learning rate, i.e the extent to which our $Q$-values are being updated in every iteration.</p>\n",
    "\n",
    "<p>The <b>$\\boldsymbol{Q}$-table</b> is a matrix where we have a row for every state and a column for every action: $64$ and $4$, respectively, when referring to the current <i>FrozenLake</i> example. It's first initialized to $0$, and then values are updated during training.</p>\n",
    "\n",
    "<p>Previously, we talked about solving this task in a na&#xEF;ve way by simply applying <i>brute force</i> using <i>random search</i>. This time, we want to apply a more sophisticated algorithm, namely $Q$-learning:\n",
    "<ul>\n",
    "    <li><code>I</code> $\\rightarrow$ Choose action $a_t$.\n",
    "    <li><code>II</code> $\\rightarrow$ Go from state $s_t$ to state $s_{t+1}$ by taking action $a_{t}$.\n",
    "    <li><code>III</code> $\\rightarrow$ For all possible $Q$-values from the state $s_{t+1}$, select the highest.\n",
    "    <li><code>IV</code> $\\rightarrow$ Update $Q$-table values using the equation from above.\n",
    "    <li><code>V</code> $\\rightarrow$ Set the next state as the current state and go back to <code>I</code> until a final state is reached (end of episode).\n",
    "</ul>\n",
    "\n",
    "This procedure is repeated for as many episodes as specified (<code>VI</code>).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba3beff",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.1 [10 Points]</b>\n",
    "    <ul>\n",
    "        <li>Implement <i>$Q$-learning</i> as outlined above (equivalently to the one discussed during the exercise).</li>\n",
    "        <li>Apply $Q$-learning on a freshly $42$-seeded <code>FrozenLakeEnv</code> instance for $50,000$ episodes, with $1,000$ delay steps, a discount factor $\\gamma=0.99$ and $\\alpha=0.1$.</li>\n",
    "        <li>Interpret the visualization of the resulting $Q$-table. What do you observe? Why do you observe this effect?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2b137e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    The following code snippet is taken from the accompanying exercise notebook. You do not need to modify it for this assignment.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be63f2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_table(q_table: np.ndarray, title: str = '') -> None:\n",
    "    \"\"\"\n",
    "    Visualize Q-table using a heatmap plot.\n",
    "    \n",
    "    :param q_table: Q-table to visualize\n",
    "    \"\"\"\n",
    "    sns.set()\n",
    "    fig, ax = plt.subplots(figsize=(20, 7))\n",
    "    sns.heatmap(data=q_table, ax=ax)\n",
    "    ax.set(xlabel='Action', ylabel='State', title=title)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(fig)\n",
    "    plt.close(fig=fig)\n",
    "\n",
    "\n",
    "def apply_q_learning(environment: gym.Env, num_episodes: int = 1000, alpha: float = 0.1, gamma: float = 1.0,\n",
    "                     animate: bool = False, delay_steps: int = 100) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Solve specified environment by applying Q-learning.\n",
    "    \n",
    "    :param environment: the environment on which to apply Q-learning\n",
    "    :param num_episodes: the total amount of episodes used to adapt the Q-table\n",
    "    :param alpha: the learning rate to be applied by Q-learning\n",
    "    :param gamma: the discount factor of future rewards\n",
    "    :param animate: animate the Q-learning process\n",
    "    :param delay_steps: the steps between each Q-table visualization (ignored if not animated)\n",
    "    :return: adapted Q-table\n",
    "    \"\"\"\n",
    "    q_table = np.zeros(shape=(environment.observation_space.n, environment.action_space.n))\n",
    "    \n",
    "    # <VI>: repeat Q-learning as long as the total amount of episodes is not yet reached.\n",
    "    for episode in range(num_episodes):\n",
    "        state = environment.reset()\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            \n",
    "            # <I>: choose next action according to current Q-table.\n",
    "            action = np.argmax(q_table[state]) \n",
    "            \n",
    "            # <II>: go from the current state to the next by applying chosen action.\n",
    "            next_state, reward, done, info = environment.step(action)\n",
    "            \n",
    "            # <III>: from all possible Q-values w.r.t. the new state, select the highest.\n",
    "            next_max = np.max(q_table[next_state])\n",
    "            \n",
    "            # <IV>: update the Q-table accordingly.\n",
    "            old_value = q_table[state, action]\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            q_table[state, action] = new_value\n",
    "\n",
    "            # <V>: update the next step with the current one.\n",
    "            state = next_state\n",
    "        \n",
    "        # Optionally visualize the current Q-table.\n",
    "        if animate and any(((episode + 1) % delay_steps == 0, (episode + 1) == num_episodes)):\n",
    "            visualize_q_table(q_table=q_table, title=f'Episode {episode + 1}')\n",
    "    \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caf1bc1",
   "metadata": {},
   "source": [
    "<p>Very likely, the $Q$-table of the previous experiment looked a little bit odd. Try to add an initial exploration to your algorithm by adapting your $Q$-learning implementation:\n",
    "    <ul>\n",
    "        <li><code>I</code> $\\rightarrow$ Get a random uniform number between $0$ and $1$. \n",
    "        <li><code>II</code> $\\rightarrow$ If the number is smaller than <code>1 - episode / (num_episodes - 1)</code>, sample a random action.\n",
    "        <li><code>III</code> $\\rightarrow$ Otherwise, choose your action as usual.\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdba83f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.2 [20 Points]</b>\n",
    "    <ul>\n",
    "        <li>Modify the <i>$Q$-learning</i> implementation from the previous tasks as outlined above (mark the corresponding code sections).</li>\n",
    "        <li>Apply $Q$-learning on a freshly $42$-seeded <code>FrozenLakeEnv</code> instance for $50,000$ episodes, with $1,000$ delay steps, discount factor $\\gamma=0.99$ and $\\alpha=0.1$.</li>\n",
    "        <li>Interpret the visualization of the resulting $Q$-table. What do you observe (compare with the previous visualization)?</li>\n",
    "        <li>Use this $Q$-learning strategy to also tackle the <code>FrozenLakeEnv</code> with <code>slippery = True</code>. Compare both $Q$-tables and discuss the differences.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116d35a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_q_learning(environment: gym.Env, num_episodes: int = 1000, alpha: float = 0.1, gamma: float = 1.0,\n",
    "                     animate: bool = False, delay_steps: int = 100) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Solve specified environment by applying Q-learning.\n",
    "    \n",
    "    :param environment: the environment on which to apply Q-learning\n",
    "    :param num_episodes: the total amount of episodes used to adapt the Q-table\n",
    "    :param alpha: the learning rate to be applied by Q-learning\n",
    "    :param gamma: the discount factor of future rewards\n",
    "    :param animate: animate the Q-learning process\n",
    "    :param delay_steps: the steps between each Q-table visualization (ignored if not animated)\n",
    "    :return: adapted Q-table\n",
    "    \"\"\"\n",
    "    raise NotImplementedError('Exchange this error with your implementation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea68ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f001f336",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.3 [15 Points]</b>\n",
    "    <ul>\n",
    "        <li>Implement a function for applying a pre-trained $Q$-table on a <code>FrozenLakeEnv</code> instance (like discussed during class).</li>\n",
    "        <li>For both <code>FrozenLakeEnv</code> instances (with <code>slippery = False</code> and <code>slippery = True</code>), conduct a guided search using the corresponding $Q$-table on a freshly $42$-seeded instance, with an animation delay of $0.1$.</li>\n",
    "        <li>Answer the following question for both settings: How many steps are necessary to reach the goal at least once and how often did an involuntary dive happen?</li>\n",
    "        <li>Compare the corresponding policies of each $Q$-table. Which property of the environment is exploited in the slippery policy to avoid involuntary dives?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c073cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f0ef7b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.4 [15 Points]</b>\n",
    "    <ul>\n",
    "        <li>Conduct a <i>$Q$-table guided search experiment</i> on the non-slippery environment, as outlined previously, using $100$ repetitions and the random seed set to $42$.</li>\n",
    "        <li>Interpret the visualization (e.g. the span of the boxes) and keep the scaling of the <i>x-axis</i> in mind.</li>\n",
    "        <li>In comparison with the <i>random search</i> experiment, how does the $Q$-table guided search perform? Discuss the results.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852027e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6817f71",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 3.5 [15 Points]</b>\n",
    "    <ul>\n",
    "        <li>Repeat the <i>$Q$-table guided search experiment</i> using $100$ repetitions and the random seed set to $42$. This time we would like to compare both policies (learned for the slippery and non-slippery environment) on the slippery environment. Hence, you have to apply the non-slippery $Q$-table to the slippery environment and also the slippery $Q$-table to the same environment. Remember to reset the seed.</li>\n",
    "        <li>Compare the number of performed steps as well as the performed dives using box- and strip-(swarm-) plots. Discuss your observations.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9be911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1067ee20f23cf75b48768bdb5f7ec1d4c21e1831c972d070ed1f98bb55bb7e57"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
